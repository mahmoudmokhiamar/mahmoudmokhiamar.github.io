---
layout: book-shelf
title: Papers/Research Articles
permalink: /papers/
nav: false
collection: books
---
> "The future depends on some graduate student who is deeply suspicious of everything I have said."
>
> — Geoffrey Hinton

## Research papers that I have read, am reading, or plan to dive into

- 🧬 **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)** – Vaswani et al.  
  *(The groundbreaking paper that introduced the Transformer architecture.)*

- 🧠 **[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)** – Devlin et al.  
  *(Revolutionized Natural Language Processing with contextual embeddings.)*

- 🧹 **[Quantifying Attention Flow in Transformers](https://arxiv.org/abs/2005.00928)** – Abnar & Zuidema  
  *(Investigates how attention weights relate to model interpretability.)*

- 🧩 **[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)** – Lewis et al.  
  *(Blends retrieval and generation to enhance factual answering.)*

- 🏥 **[The Why and How of Explainable AI: Insights from the Medical Domain](https://arxiv.org/abs/1712.09923)** – Holzinger et al.  
  *(Focused on transparency and trust in AI-assisted healthcare.)*

- 🍺 **[Having Beer after Prayer? Measuring Cultural Bias in Large Language Models](https://arxiv.org/pdf/2305.14456)** – Naous et al.  
  *(Investigates cultural bias across societal contexts in LLMs.)*
  

---