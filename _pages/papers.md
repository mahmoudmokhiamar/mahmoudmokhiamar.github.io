---
layout: book-shelf
title: Papers/Research Articles
permalink: /papers/
nav: false
collection: books
---
> "The future depends on some graduate student who is deeply suspicious of everything I have said."
>
> â€” Geoffrey Hinton

## Research papers that I have read, am reading, or plan to dive into

- ğŸ§¬ **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)** â€“ Vaswani et al.  
  *(The groundbreaking paper that introduced the Transformer architecture.)*

- ğŸ§  **[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)** â€“ Devlin et al.  
  *(Revolutionized Natural Language Processing with contextual embeddings.)*

- ğŸ§¹ **[Quantifying Attention Flow in Transformers](https://arxiv.org/abs/2005.00928)** â€“ Abnar & Zuidema  
  *(Investigates how attention weights relate to model interpretability.)*

- ğŸ§© **[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)** â€“ Lewis et al.  
  *(Blends retrieval and generation to enhance factual answering.)*

- ğŸ¥ **[The Why and How of Explainable AI: Insights from the Medical Domain](https://arxiv.org/abs/1712.09923)** â€“ Holzinger et al.  
  *(Focused on transparency and trust in AI-assisted healthcare.)*

- ğŸº **[Having Beer after Prayer? Measuring Cultural Bias in Large Language Models](https://arxiv.org/pdf/2305.14456)** â€“ Naous et al.  
  *(Investigates cultural bias across societal contexts in LLMs.)*
  

---